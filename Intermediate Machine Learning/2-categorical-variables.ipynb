{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex3 import *\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:31:26.656200Z","iopub.execute_input":"2022-12-08T05:31:26.656577Z","iopub.status.idle":"2022-12-08T05:31:26.729273Z","shell.execute_reply.started":"2022-12-08T05:31:26.656506Z","shell.execute_reply":"2022-12-08T05:31:26.727644Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Setup Complete\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id') \nX_test = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:31:31.142319Z","iopub.execute_input":"2022-12-08T05:31:31.142640Z","iopub.status.idle":"2022-12-08T05:31:32.286128Z","shell.execute_reply.started":"2022-12-08T05:31:31.142617Z","shell.execute_reply":"2022-12-08T05:31:32.284931Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:31:41.693569Z","iopub.execute_input":"2022-12-08T05:31:41.693951Z","iopub.status.idle":"2022-12-08T05:31:41.951064Z","shell.execute_reply.started":"2022-12-08T05:31:41.693923Z","shell.execute_reply":"2022-12-08T05:31:41.949205Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 分类型变量处理\n1. 不含有用信息时可以直接删除该列；\n2. Ordinal encoding（有序变量，例如：\"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3)）\n3. One-Hot Encoding（名义变量，变量有超过15个取值时表现不佳）","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Drop columns with categorical data\n\nYou'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  ","metadata":{}},{"cell_type":"code","source":"# Fill in the lines below: drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n# Check your answers\nstep_1.check()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:32:11.208290Z","iopub.execute_input":"2022-12-08T05:32:11.208637Z","iopub.status.idle":"2022-12-08T05:32:11.225566Z","shell.execute_reply.started":"2022-12-08T05:32:11.208611Z","shell.execute_reply":"2022-12-08T05:32:11.223677Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Drop\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:32:20.266571Z","iopub.execute_input":"2022-12-08T05:32:20.266932Z","iopub.status.idle":"2022-12-08T05:32:21.353864Z","shell.execute_reply.started":"2022-12-08T05:32:20.266908Z","shell.execute_reply":"2022-12-08T05:32:21.352433Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"MAE from Approach 1 (Drop categorical variables):\n17837.82570776256\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Before jumping into ordinal encoding, we'll investigate the dataset.  Specifically, we'll look at the `'Condition2'` column.  The code cell below prints the unique entries in both the training and validation sets.","metadata":{}},{"cell_type":"code","source":"print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:32:44.995713Z","iopub.execute_input":"2022-12-08T05:32:44.996072Z","iopub.status.idle":"2022-12-08T05:32:45.003612Z","shell.execute_reply.started":"2022-12-08T05:32:44.996047Z","shell.execute_reply":"2022-12-08T05:32:45.002338Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n\nUnique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 2: Ordinal encoding\n\n### Part A\n\nIf you now write code to: \n- fit an ordinal encoder to the training data, and then \n- use it to transform both the training and validation data, \n\n当某个类别出现在验证集，但训练集中没有时，会引发错误","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nstep_2.a.check()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:33:28.117407Z","iopub.execute_input":"2022-12-08T05:33:28.117749Z","iopub.status.idle":"2022-12-08T05:33:28.126761Z","shell.execute_reply.started":"2022-12-08T05:33:28.117726Z","shell.execute_reply":"2022-12-08T05:33:28.125574Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"2.1_LabelA\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\nFitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value **that appears in the training data**. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.  Notice that the `'Condition2'` column in the validation data contains the values `'RRAn'` and `'RRNn'`, but these don't appear in the training data -- thus, if we try to use an ordinal encoder with scikit-learn, the code will throw an error.","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\nFitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value **that appears in the training data**. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.  Notice that the `'Condition2'` column in the validation data contains the values `'RRAn'` and `'RRNn'`, but these don't appear in the training data -- thus, if we try to use an ordinal encoder with scikit-learn, the code will throw an error."},"metadata":{}}]},{"cell_type":"markdown","source":"解决办法：（1）write a custom ordinal encoder to deal with new categories. （2）The simplest approach, however, is to drop the problematic categorical columns.  \n\nRun the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely ordinal encoded are stored in `good_label_cols`.","metadata":{}},{"cell_type":"code","source":"# 获取bad_label_cols和good_label_cols\n\n# 获得所有分类型变量\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# 找出可以被安全转换的分类型特征\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# 不可以被安全转换的分类型特征（验证集里的某些类别并未包含在训练集中）\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:36:25.222094Z","iopub.execute_input":"2022-12-08T05:36:25.222427Z","iopub.status.idle":"2022-12-08T05:36:25.238288Z","shell.execute_reply.started":"2022-12-08T05:36:25.222402Z","shell.execute_reply":"2022-12-08T05:36:25.236701Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Categorical columns that will be ordinal encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nCategorical columns that will be dropped from the dataset: ['Condition2', 'RoofMatl', 'Functional']\n","output_type":"stream"}]},{"cell_type":"code","source":"# 删掉bad_label_cols，对good_label_cols做OrdinalEncoder\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\nmyOE = OrdinalEncoder()\nlabel_X_train[good_label_cols] = myOE.fit_transform(label_X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = myOE.fit_transform(label_X_valid[good_label_cols])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:38:38.600794Z","iopub.execute_input":"2022-12-08T05:38:38.601136Z","iopub.status.idle":"2022-12-08T05:38:38.647508Z","shell.execute_reply.started":"2022-12-08T05:38:38.601111Z","shell.execute_reply":"2022-12-08T05:38:38.646469Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n\n# MAE from Approach 1 (Drop categorical variables):17837.82570776256\n# MAE from Approach 2 (Ordinal Encoding):17274.866860730595","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:38:48.108315Z","iopub.execute_input":"2022-12-08T05:38:48.108695Z","iopub.status.idle":"2022-12-08T05:38:49.444982Z","shell.execute_reply.started":"2022-12-08T05:38:48.108670Z","shell.execute_reply":"2022-12-08T05:38:49.444071Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"MAE from Approach 2 (Ordinal Encoding):\n17274.866860730595\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  ","metadata":{}},{"cell_type":"code","source":"# 获取每个分类特征的特征数\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:39:33.085440Z","iopub.execute_input":"2022-12-08T05:39:33.086817Z","iopub.status.idle":"2022-12-08T05:39:33.098835Z","shell.execute_reply.started":"2022-12-08T05:39:33.086762Z","shell.execute_reply":"2022-12-08T05:39:33.098119Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[('Street', 2),\n ('Utilities', 2),\n ('CentralAir', 2),\n ('LandSlope', 3),\n ('PavedDrive', 3),\n ('LotShape', 4),\n ('LandContour', 4),\n ('ExterQual', 4),\n ('KitchenQual', 4),\n ('MSZoning', 5),\n ('LotConfig', 5),\n ('BldgType', 5),\n ('ExterCond', 5),\n ('HeatingQC', 5),\n ('Condition2', 6),\n ('RoofStyle', 6),\n ('Foundation', 6),\n ('Heating', 6),\n ('Functional', 6),\n ('SaleCondition', 6),\n ('RoofMatl', 7),\n ('HouseStyle', 8),\n ('Condition1', 9),\n ('SaleType', 9),\n ('Exterior1st', 15),\n ('Exterior2nd', 16),\n ('Neighborhood', 25)]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 3: Investigating cardinality\n","metadata":{}},{"cell_type":"markdown","source":"对于具有多行的大型数据集，一次热编码可以极大地扩展数据集的大小。由于这个原因，我们通常只使用一个具有相对较低基数的热编码列。然后，可以从数据集中删除高基数列，或者我们可以使用序数编码。\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the ordinal encoding, how many entries are added?  \n\nUse your answers to fill in the lines below.","metadata":{}},{"cell_type":"code","source":"# Fill in the line below: How many entries are added to the dataset by \n# replacing the column with a one-hot encoding?\nOH_entries_added = 10000*100-10000\n\n# Fill in the line below: How many entries are added to the dataset by\n# replacing the column with an ordinal encoding?\nlabel_entries_added = 0\n\n# Check your answers\nstep_3.b.check()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:44:21.293545Z","iopub.execute_input":"2022-12-08T05:44:21.293912Z","iopub.status.idle":"2022-12-08T05:44:21.304837Z","shell.execute_reply.started":"2022-12-08T05:44:21.293886Z","shell.execute_reply":"2022-12-08T05:44:21.303618Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 1, \"questionId\": \"3.2_CardinalityB\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, you'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset.","metadata":{}},{"cell_type":"code","source":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:45:16.641542Z","iopub.execute_input":"2022-12-08T05:45:16.641872Z","iopub.status.idle":"2022-12-08T05:45:16.652815Z","shell.execute_reply.started":"2022-12-08T05:45:16.641846Z","shell.execute_reply":"2022-12-08T05:45:16.651941Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nCategorical columns that will be dropped from the dataset: ['Exterior2nd', 'Exterior1st', 'Neighborhood']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 4: One-hot encoding\n\nUse the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# 对类别数较少的分类型变量进行OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1) \nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\n# Check your answer\nstep_4.check()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:50:18.525469Z","iopub.execute_input":"2022-12-08T05:50:18.525846Z","iopub.status.idle":"2022-12-08T05:50:18.561294Z","shell.execute_reply.started":"2022-12-08T05:50:18.525822Z","shell.execute_reply":"2022-12-08T05:50:18.560029Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"4_OneHot\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T05:50:24.650299Z","iopub.execute_input":"2022-12-08T05:50:24.650609Z","iopub.status.idle":"2022-12-08T05:50:26.297690Z","shell.execute_reply.started":"2022-12-08T05:50:24.650586Z","shell.execute_reply":"2022-12-08T05:50:26.296852Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"MAE from Approach 3 (One-Hot Encoding):\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"17525.345719178084\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"}]}]}